## 基于实例学习
特点：只存储样本
分类：检索训练集中相似的样本
与其它算法的关键差异：为每个分类实例建立目标函数
优点：
用多个简单局部函数逼近复杂的目标函数 (局部)
缺点：
分类代价高，高效索引训练样本
相似实例可能会距离远
有时目标概念仅依赖于多属性中的几个属性
## k-近邻算法
![输入图片说明](/imgs/2025-11-21/2zW9csQJLI8oxrhD.png)
![输入图片说明](/imgs/2025-11-21/Sw438LNdoAPEGxcG.png)
对于连续值的情况，可以如下处理
![输入图片说明](/imgs/2025-11-21/aoESVm0szlvXeKqG.png)
## 距离加权最近邻算法
k-近邻的改进，根据查询点xq的距离，将较大的权值赋值给较近的近邻![输入图片说明](/imgs/2025-11-21/IFQdnQOi7UhJg6LS.png)
如果查询点正好匹配某一训练样例，则直接令其值为该样例，如果有多个匹配样例，则取最普遍的值
类似的方式对于实值目标函数
![输入图片说明](/imgs/2025-11-21/uuuA3ngYwHghmicC.png)
还有一种改进方法：全局法，考虑全局所有点的距离加权查询，缺点是分类时间长
## 对k-近邻算法的说明
距离加权k-NN 对噪声有鲁棒性
可以平滑掉孤立的噪声训练样本的影响.
k-近邻的归纳偏置：
xq 的类别与它附近的实例的类别相似.
 k-近邻实践中的问题: 维度灾难（curse of dimensionality）
基于全部属性计算距离 
实际中类别相关属性只占少数 
误导k-近邻分类器
克服维度灾难的方法:
对属性加权,  相当于按比例缩放坐标轴，伸展第j个坐标轴 zj倍，用交叉验证，选取最佳zj
清除最不相关属性（ Zj=0），采用基于留一法 (leave-one-out)的交叉验证 选择属性子集
用变化的值伸展属性坐标轴，自由度高，但也增加了过拟合风险
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTg0NTUwMzMzMCwxMzczOTc3NDE5LDg1MD
Y2NjkwNl19
-->