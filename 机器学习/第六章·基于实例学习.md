## 基于实例学习
特点：只存储样本
分类：检索训练集中相似的样本
与其它算法的关键差异：为每个分类实例建立目标函数
优点：
用多个简单局部函数逼近复杂的目标函数 (局部)
缺点：
分类代价高，高效索引训练样本
相似实例可能会距离远
有时目标概念仅依赖于多属性中的几个属性
## k-近邻算法
![输入图片说明](/imgs/2025-11-21/2zW9csQJLI8oxrhD.png)
![输入图片说明](/imgs/2025-11-21/Sw438LNdoAPEGxcG.png)
对于连续值的情况，可以如下处理
![输入图片说明](/imgs/2025-11-21/aoESVm0szlvXeKqG.png)
## 距离加权最近邻算法
k-近邻的改进，根据查询点xq的距离，将较大的权值赋值给较近的近邻![输入图片说明](/imgs/2025-11-21/IFQdnQOi7UhJg6LS.png)
如果查询点正好匹配某一训练样例，则直接令其值为该样例，如果有多个匹配样例，则取最普遍的值
类似的方式对于实值目标函数
![输入图片说明](/imgs/2025-11-21/uuuA3ngYwHghmicC.png)
还有一种改进方法：全局法，考虑全局所有点的距离加权查询，缺点是分类时间长
## 对k-近邻算法的说明
距离加权k-NN 对噪声有鲁棒性
可以平滑掉孤立的噪声训练样本的影响.
k-近邻的归纳偏置：
xq 的类别与它附近的实例的类别相似.
 k-近邻实践中的问题: 维度灾难（curse of dimensionality）
基于全部属性计算距离 
实际中类别相关属性只占少数 
误导k-近邻分类器
克服维度灾难的方法:
对属性加权,  相当于按比例缩放坐标轴，伸展第j个坐标轴 zj倍，用交叉验证，选取最佳zj
清除最不相关属性（ Zj=0），采用基于留一法 (leave-one-out)的交叉验证 选择属性子集
用变化的值伸展属性坐标轴，自由度高，但也增加了过拟合风险
## 术语说明
![输入图片说明](/imgs/2025-11-21/VFbD8jrv6GwNxTX6.png)
## 局部加权回归
最近邻方法是在单一查询点上逼近目标函数，局部回归加权为该方法的推广。通过在环绕xq查询点的局部区域内为目标函数f建立明确的逼近
局部的含义是目标函数的逼近仅根据查询点附近的数据，加权的含义是因为每一个训练样例的贡献是由它与查询点之间的距离加权的
![输入图片说明](/imgs/2025-11-21/0w6biHRtNBtdmrus.png)
### 线性回归
![输入图片说明](/imgs/2025-11-21/XUVXgzc0ZXSpiwbh.png)![输入图片说明](/imgs/2025-11-21/Dij69uORiAfqRHKK.png)
第三种的结合其实就是考虑对k近邻的点距离加权计算误差值![输入图片说明](/imgs/2025-11-21/i2QNIIsW58boieix.png)
可用常量、线性函数、二次函数局部近似目标函数，更复杂的函数形式不常见，因为：
1.代价高
2.简单函数的近似效果已相当好
## 径向基函数
![输入图片说明](/imgs/2025-11-21/lzOItMkgK3saPfQM.png)
K随着距离增大而减小，k是用户指定的常量，用于指定要包含的核函数的数量
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE2ODkzMjMyOTEsMTE5OTIzNjUzOSwxMz
czOTc3NDE5LDg1MDY2NjkwNl19
-->