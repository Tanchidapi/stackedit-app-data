## 定义
决策树是一种逼近离散值目标函数的方法
![输入图片说明](/imgs/2025-11-20/nYKoMgavlt6QXj69.png)
## 适用范围
实例是<属性，值>对
目标函数有离散的输出
可能需要析取的描述
可能包含错误的训练集
包含缺少属性的实例的训练集
适用于分类问题，将实例识别为离散的可能
## 核心算法：ID3
ID3算法通过自顶向下构造决策树学习，利用了贪婪搜索遍历可能的决策树空间
![输入图片说明](/imgs/2025-11-20/UP2DJDgIopDRdrc1.png)
## 信息熵与信息增益
ID3算法的核心之一是要判断出哪个属性是（当前）最佳的分类属性，用信息熵和信息增益衡量
信息熵刻画了样例集的纯度（对S中任意成员类别编码所需的最小位数），信息增益度量某个属性的分类能力
![输入图片说明](/imgs/2025-11-20/8eOGpoK0JtW2x7Nb.png)
![输入图片说明](/imgs/2025-11-20/KFwLobzr23Avbq8l.png)
## 决策树学习的假设空间搜索
ID3的优势：避免了搜索不完整的假设空间，避免了假设空间可能不包含目标函数的情况。同时对样例错误的敏感性低，易扩展到处理含有噪声的训练数据
不足：ID3仅维护单一的当前假设，不能判断有多少其他决策树和现有数据一致。基本ID3算法不回溯，容易收敛到局部最优
## 决策树的归纳偏置
ID3一般从决策树中选择：1.较短的树 2.信息增益高的属性离根节点较近的树
![输入图片说明](/imgs/2025-11-20/wonzBOP3thzpdIoz.png)
ID3的归纳偏置是对某种假设胜过其他假设的一种优选，对最终可列举的假设没有硬性吸纳之，称为优选偏置
候选消除算法的偏置是对待考虑假设的一种限定，称之为限定偏置
通常优选偏置比限定偏置更符合泛化的需要，它允许学习器工作在完整的假设空间上。
## 过拟合
![输入图片说明](/imgs/2025-11-20/RTSOb4oMoLZAACAC.png)
训练集中的随机噪声、少数实例和叶节点的关联性都可能导致过拟合
可以通过及时停止对树的增长和后修剪法，即允许树过拟合，然后再对树进行修剪来防止过拟合
修剪方法：
错误率降低修剪：删除以该节点为根的子树，使其变为叶节点，指定类别为最常见的训练样例的类别，如果修剪后的性能不比原来的差，则重复直到修剪有害
规则后修剪：对于过拟合的决策树，将每个从根到叶子的路径创建一条规则，删除可以提高规则估计精度的前件，对修剪后的规则按估计精度排序，按顺序对后续实例分类
估计规则的精度：
使用验证集
基于训练集本身：
使用悲观估计弥补了训练数据有利于当前规则的估计偏置
估计过程(Accuracys 样本精度, AccuracyD 估计精度,binomial distribution)
计算规则的样本精度Accuracys
计算二项分布下的标准差(假设Accuracys服从二项式分布)
对于给定的置信水平，采用下限估计作为规则估计精度AccuracyD
对于大数据集，悲观估计非常接近观察精度，在统计上是无效的，但在实践中是有用的
## 合并连续值属性
连续值属性通过动态定义新的离散值属性进行处理
定义基于阈值的布尔属性，选择具有最大信息增益的阈值C：根据A对样例进行排序，找到目标属性不同的相邻样例，在A值对应区间创建一组候选阈值(取中间值)，可以证明，使信息增益最大的c位于该边界，最后计算各阈值的信息增益
## 属性选择的其他度量
信息增益度量有一个内在的偏置，其偏袒较多值的属性。
增益比率通过引入一个称为分裂信息的项惩罚类似属性，用于衡量分裂数据的广度和均匀性
![输入图片说明](/imgs/2025-11-20/ENuHx48VXyuXoEI4.png)
## 缺少属性值的训练样例

## 例题 · playtennis
![输入图片说明](/imgs/2025-11-20/jlvhj8CX7wGq395P.png)
![输入图片说明](/imgs/2025-11-20/HTjM0zom9TJHoWW2.png)
![输入图片说明](/imgs/2025-11-20/AsHU0FmpAYVyYuCr.png)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ4MzI4OTQyMiw1MjYxNzQ1MzYsLTE0ND
MzMjIwMDIsLTE3ODYxNzI3NSwxMzcwNzQ0MjEyLDcyOTQyMzk3
Niw3ODk0MjE5NCwtMTcxMzU0OTAyLDczODY3Mjc2NywtMjA4OD
c0NjYxMl19
-->